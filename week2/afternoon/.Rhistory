install.packages("devtools")
setwd("~/GitHub/stardog/week2/afternoon")
source('pttTestFunction.R')
install.packages(xml2)
install.packages(xml2)
source('pttTestFunction.R')
install.packages(xml2)
installed.packages(xml2)
setwd("~/GitHub/stardog/week2/afternoon")
install.packages(xm12)
install.packages(xm12)
install.packages(xml2)
source('pttTestFunction.R')
install.packages(xml2)
install.packages("xml2")
source('pttTestFunction.R')
install.packages("tmcn")
source('pttTestFunction.R')
install.packages("rvest")
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38820:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38820:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
pttTestFunction(URL[1], filename[1])
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38830:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38830:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38830:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/Gossiping/index.html
id = c(38830:38834)
URL = paste0("https://www.ptt.cc/bbs/Gossiping/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
mapply(pttTestFunction,
URL = URL, filename = filename)
pttTestFunction(URL[1], filename[1])
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/marvel/index.html
id = c(2170:2175)
URL = paste0("https://www.ptt.cc/bbs/marvel/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
install.packages("tmcn")
install.packages("tmcn")
install.packages("NLP")
install.packages("tm")
install.packages("jiebaRD")
install.packages("jiebaR")
install.packages("RColorbrewer")
install.packages("RColorBrewer")
install.packages("worldcloud")
install.packages("wordcloud")
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "看板")
docs <- tm_map(docs, toSpace, "作者")
docs <- tm_map(docs, toSpace, "發信站")
docs <- tm_map(docs, toSpace, "批踢踢實業坊")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
docs <- tm_map(docs, toSpace, "推")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
library(jiebaR)
cutter <- worker()
cutter["真是詭異，好毛喔"]
new_user_word(cutter,'好毛',"n")
cutter["真是詭異，好毛喔"]
new_user_word(cutter,'暗網',"n")
new_user_word(cutter,'詭異',"n")
new_user_word(cutter,'獵奇',"n")
new_user_word(cutter,'敲碗',"n")
new_user_word(cutter,'戀童',"n")
new_user_word(cutter,'恐怖',"n")
new_user_word(cutter,'媽佛',"n")
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "看板")
docs <- tm_map(docs, toSpace, "作者")
docs <- tm_map(docs, toSpace, "發信站")
docs <- tm_map(docs, toSpace, "批踢踢實業坊")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, toSpace, "敲碗")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs
cutter["真是詭異，好毛喔"]
library(jiebaR)
cutter <- worker()
cutter
cutter["真是詭異，好毛喔"]
new_user_word(cutter,'好毛',"n")
new_user_word(cutter,'媽佛',"n")
new_user_word(cutter,'獵奇',"n")
new_user_word(cutter,'敲碗',"n")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
docs <- tm_map(docs, toSpace, "推")
kable(head(freqFrame), format = "markdown")
docs <- tm_map(docs, toSpace, "推")
kable(head(freqFrame), format = "markdown")
docs <- tm_map(docs, toSpace, "推")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
docs <- tm_map(docs, toSpace, "了")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
new_user_word(cutter,'就在',"n")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
cutter["他就在這裡"]
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(jiebaR)
cutter <- worker()
cutter
cutter["他就在這裡"]
new_user_word(cutter,'好毛',"n")
new_user_word(cutter,'媽佛',"n")
new_user_word(cutter,'獵奇',"n")
new_user_word(cutter,'敲碗',"n")
new_user_word(cutter,'就在',"n")
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "看板")
docs <- tm_map(docs, toSpace, "作者")
docs <- tm_map(docs, toSpace, "發信站")
docs <- tm_map(docs, toSpace, "批踢踢實業坊")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, toSpace, "推")
docs <- tm_map(docs, toSpace, "敲碗")
docs <- tm_map(docs, toSpace, "了")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
new_user_word(cutter,'自殺',"n")
library(jiebaR)
cutter <- worker()
cutter
cutter["他就在這裡"]
new_user_word(cutter,'好毛',"n")
new_user_word(cutter,'媽佛',"n")
new_user_word(cutter,'獵奇',"n")
new_user_word(cutter,'敲碗',"n")
new_user_word(cutter,'就在',"n")
new_user_word(cutter,'自殺',"n")
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/marvel/index.html
id = c(2170:2175)
URL = paste0("https://www.ptt.cc/bbs/marvel/index", id, ".html")
source('pttTestFunction.R')
#https://www.ptt.cc/bbs/marvel/index.html
id = c(2170:2175)
URL = paste0("https://www.ptt.cc/bbs/marvel/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
library(jiebaR)
cutter <- worker()
cutter
cutter["他就在這裡"]
new_user_word(cutter,'好毛',"n")
new_user_word(cutter,'媽佛',"n")
new_user_word(cutter,'獵奇',"n")
new_user_word(cutter,'敲碗',"n")
new_user_word(cutter,'就在',"n")
new_user_word(cutter,'自殺',"n")
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "看板")
docs <- tm_map(docs, toSpace, "作者")
docs <- tm_map(docs, toSpace, "發信站")
docs <- tm_map(docs, toSpace, "批踢踢實業坊")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, toSpace, "推")
docs <- tm_map(docs, toSpace, "敲碗")
docs <- tm_map(docs, toSpace, "了")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame), format = "markdown")
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
